# Environment
env_type: 'gym' # gym, custom
env_name: 'Freeway-ram-v0' # if custom anyname is OK

# Network
nn_mod: 'dist_dqn' # must match with network_models counts
network_models: ['DistDQN']
loss_fn: 'dist-cross-entropy' # actor-critic, dist-cross-entropy
action_space: 3

# Train
optimizer: 'adam'
# learning_rate: 3.e-3 # for no target nn SGD
# learning_rate: 6.e-4 # for no target nn
learning_rate: 1.e-4 # for target nn
epochs: 2000
multi_procs: False # use multi processing
num_multi_procs: 1

# Agent
## run
prob_q: True # wheter q value is distributed probability for vqlue_nn
policy: 'greedy' #greedy, softmax, mix
epsilon: 0.2 # in Episode
epsilon_min: 0.05 # in Episode
epsilon_thrs: 200 # in Episode

## update parameter
flip_res: False # 
test_interval: 100
test_qty: 10
sampled_batch: True # with shuffling
batch_size: 50
target_nn: True # for value fn, retarded loss reduction and training
t_update: 25 # for value fn

# Episode
ep_lim: True # episode 'n step' mode
ignore_done: False # prior than ep_lim
epbuff_size: 200 # 500 can contain success experience
reset_epbuff: True
exp_priority: True # experience priority
priority_level: 5 # default 5

# Dist DQN
dist_network_dim: [128, 100, 25, 51] # in network
dist_gamma: 0.8 # in agent
bootstrap: False # in agent, False more stable
support_div: 51 # in utils
dist_support_limit: [-10., 10.] # in utils





